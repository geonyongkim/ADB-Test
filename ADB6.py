import streamlit as st
import pandas as pd
import numpy as np
from nptdms import TdmsFile
import plotly.express as px
import plotly.graph_objects as go
import io
import re
import os

# --- 1. Configuration ---
# VBScriptì—ì„œ ì¶”ì¶œí•œ ì½”ìŠ¤ë³„ ì¡°ë„ê³„ ì›ì  ì¢Œí‘œ
ZERO_MAP = {
    "100L": (357.220, 0.864),
    "230L": (357.645, 1.076),
    "230R": (360.405, 1.031),
    "370L": (357.850, 1.174),
    "370R": (360.202, 1.110),
    "230LP": (357.645, 1.076), # Preceding
    "ST": (None),  # Oncoming - PosLocalX ê¸°ì¤€
    "STP": None, # Preceding - PosLocalX ê¸°ì¤€
}

ILLUMINANCE_SAMPLING_RATE = 201  # Hz
DISTANCE_GRID_INTERVAL = 0.1  # meters

# ê³ ì • ì‹œê°„ ì˜¤í”„ì…‹
TIME_OFFSET_SECONDS = 32381

# --- 2. Modular Functions ---

def find_and_rename_columns(df, column_map):
    """DataFrameì—ì„œ ì§€ì •ëœ íŒ¨í„´ì˜ ì»¬ëŸ¼ì„ ì°¾ì•„ í‘œì¤€ ì´ë¦„ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤."""
    rename_dict = {}
    for standard_name, pattern in column_map.items():
        for col in df.columns:
            if re.search(pattern, col, re.IGNORECASE):
                rename_dict[col] = standard_name
                break
    return df.rename(columns=rename_dict)

def load_vehicle_csv(uploaded_file):
    """ì£¼í–‰ ë°ì´í„° CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  Timestamp ìƒì„± ë° ì»¬ëŸ¼ í‘œì¤€í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    if uploaded_file is None: return None
    try:
        try:
            df = pd.read_csv(uploaded_file)
        except UnicodeDecodeError:
            uploaded_file.seek(0)
            df = pd.read_csv(uploaded_file, encoding='cp949')
            st.info("CSV íŒŒì¼ì„ 'cp949' ì¸ì½”ë”©ìœ¼ë¡œ ë‹¤ì‹œ ì½ì—ˆìŠµë‹ˆë‹¤.")

        df_processed = df.copy()

        # ì£¼ìš” ì»¬ëŸ¼ í‘œì¤€í™”
        column_map = {
            "Date": r"Date.*",
            "Time": r"Time.*\(UTC",
            "TimeFromStart": r"TimeFromStart.*",
            "PosLocalX": r"PosLocalX.*",
            "PosLocalY": r"PosLocalY.*",
            "Pitch": r"AnglePitch.*", # Pitch ë°ì´í„° ì¶”ê°€
        }
        df_processed = find_and_rename_columns(df_processed, column_map)

        # Timestamp ìƒì„±
        if "Date" in df_processed.columns and "Time" in df_processed.columns:
            df_processed["Timestamp"] = pd.to_datetime(df_processed["Date"] + " " + df_processed["Time"], errors='coerce')
        else:
            st.warning("CSV íŒŒì¼ì— ë‚ ì§œ/ì‹œê°„ ì •ë³´ê°€ ì—†ì–´ Timestampë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        st.success("ì£¼í–‰ ë°ì´í„°(CSV) ë¡œë“œ ë° ì „ì²˜ë¦¬ ì„±ê³µ")
        return df_processed

    except Exception as e:
        st.error(f"CSV íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None

def load_illuminance_tdms(uploaded_file):
    """ì¡°ë„ TDMS íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì±„ë„ í‘œì¤€í™” ë° Timestampë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if uploaded_file is None: return None
    try:
        with io.BytesIO(uploaded_file.getvalue()) as buffer:
            tdms_file = TdmsFile(buffer)
        
        all_channels = tdms_file.as_dataframe()

        # í‘œì¤€ ì¡°ë„ ì±„ë„ DataFrame ìƒì„±
        standard_channels = [f"Middle Gain {i}" for i in range(1, 11)]
        df_illuminance = pd.DataFrame()

        # ì‹œê°„ ë°ì´í„° ì¶”ì¶œ
        time_data = None
        timestamp_col_name = None

        # 1. 'timestamp' ì´ë¦„ì˜ ì±„ë„ì„ ë¨¼ì € ì°¾ì•„ë´…ë‹ˆë‹¤.
        for col in all_channels.columns:
            if 'timestamp' in col.lower():
                timestamp_col_name = col
                break
        
        if timestamp_col_name:
            st.info(f"'{timestamp_col_name}' ì±„ë„ì„ ì‹œê°„ ì •ë³´ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            time_data = all_channels[timestamp_col_name]
        else:
            # 2. 'timestamp' ì±„ë„ì´ ì—†ìœ¼ë©´ ê¸°ì¡´ì˜ time_track() ë°©ì‹ì„ ì‹œë„í•©ë‹ˆë‹¤.
            st.info("'timestamp' ì±„ë„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. time_track() ë©”ì„œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
            for group in tdms_file.groups():
                for channel in group.channels():
                    try:
                        time_data = channel.time_track()
                        break
                    except KeyError:
                        continue
                if time_data is not None:
                    break
        
        if time_data is None:
            st.error("TDMS íŒŒì¼ì—ì„œ ì‹œê°„ ì±„ë„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        df_illuminance['Timestamp'] = pd.to_datetime(time_data)

        # Middle Gain ì±„ë„ íƒì§€ ë° í‘œì¤€í™”
        found_channels = 0
        for i in range(1, 11):
            standard_name = f"Middle Gain {i}"
            # ë‹¤ì–‘í•œ ì±„ë„ ì´ë¦„ í˜•ì‹ì— ëŒ€ì‘í•˜ê¸° ìœ„í•œ ì •ê·œì‹
            pattern = re.compile(f".*mid(dle)?.*gain.*{i}.*", re.IGNORECASE)
            
            found = False
            for col in all_channels.columns:
                if pattern.match(col):
                    df_illuminance[standard_name] = all_channels[col]
                    found = True
                    found_channels += 1
                    break
            if not found:
                df_illuminance[standard_name] = np.nan # ì±„ë„ì´ ì—†ìœ¼ë©´ NaNìœ¼ë¡œ ì±„ì›€

        if found_channels == 0:
            st.warning("TDMS íŒŒì¼ì—ì„œ 'Middle Gain' ì±„ë„ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        else:
            st.success(f"ì¡°ë„ ë°ì´í„°(TDMS) ë¡œë“œ ë° {found_channels}ê°œ ì±„ë„ í‘œì¤€í™” ì„±ê³µ")

        return df_illuminance
        
    except Exception as e:
        st.error(f"TDMS íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        return None


def synchronize_data(df_vehicle, df_illuminance, offset):
    """ì°¨ëŸ‰ê³¼ ì¡°ë„ ë°ì´í„°ë¥¼ ì‹œê°„ ë™ê¸°í™”í•©ë‹ˆë‹¤."""
    if df_vehicle is None or df_illuminance is None:
        return None

    df_v = df_vehicle.copy()
    df_i = df_illuminance.copy()

    df_i["Timestamp"] = df_i["Timestamp"] + pd.to_timedelta(offset, unit='s')
    st.info(f"{offset}ì´ˆ ì˜¤í”„ì…‹ ì ìš© ì™„ë£Œ.")

    df_v = df_v.sort_values("Timestamp").reset_index(drop=True)
    df_i = df_i.sort_values("Timestamp").reset_index(drop=True)

    st.info("ê°€ì¥ ê°€ê¹Œìš´ ì‹œê°„ì˜ ì¡°ë„ ë°ì´í„°ë¥¼ ë§¤ì¹­í•©ë‹ˆë‹¤.")
    df_synced = pd.merge_asof(
        left=df_v,
        right=df_i,
        on="Timestamp",
        direction="nearest"
    )

    original_samples = len(df_v)
    st.info("ë³‘í•© í›„ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬: í•„ìˆ˜ ì¢Œí‘œ(PosLocalX/Y) ë˜ëŠ” ëª¨ë“  ì¡°ë„ ì±„ë„ì´ ì—†ëŠ” í–‰ì„ ì œê±°í•©ë‹ˆë‹¤.")
    df_synced.dropna(subset=['PosLocalX', 'PosLocalY'], inplace=True)
    mid_gain_cols = [f'Middle Gain {i}' for i in range(1, 11)]
    df_synced.dropna(subset=mid_gain_cols, how='all', inplace=True)
    synced_samples = len(df_synced)
    
    st.success(f"ë°ì´í„° ë™ê¸°í™” ì™„ë£Œ. ìœ íš¨ ìƒ˜í”Œ: {synced_samples} / {original_samples}")
    
    if synced_samples < 2:
        st.error("ë™ê¸°í™” í›„ ë°ì´í„° ìƒ˜í”Œì´ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. ì‹œê°„ ì˜¤í”„ì…‹ ë˜ëŠ” ì…ë ¥ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None

    return df_synced

def calculate_distance(df, zero_point):
    """ì¡°ë„ê³„ ì›ì  ëŒ€ë¹„ ì°¨ëŸ‰ì˜ ì§ì„ ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    if df is None: return None
    df_processed = df.copy()
    if zero_point:
        x0, y0 = zero_point
        raw_distance = np.sqrt((df_processed['PosLocalX'] - x0)**2 + (df_processed['PosLocalY'] - y0)**2)
    else:
        # For straight sections (e.g., ST, STP) where zero_point is None,
        # calculate distance relative to PosLocalX = 359.
        raw_distance = (df_processed['PosLocalX'] - 359).abs()
    df_processed['Distance'] = raw_distance
    st.success("ê±°ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤.")
    return df_processed
def get_evaluation_ranges(course, scenario):
    """í”„ë¡¬í”„íŠ¸ì— ëª…ì‹œëœ ì½”ìŠ¤ë³„ í‰ê°€ ê±°ë¦¬ ë²”ìœ„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if scenario == 'Oncoming':
        if course == 'ST': return [(15, 220)]
        if course == '100L': return [(15, 59.9)]
        if course == '230L': return [(15, 150)]
        if course == '370L': return [(15, 220)]
        if course == '230R': return [(15, 59.9)]
        if course == '370R': return [(15, 70)]
    elif scenario == 'Preceding':
        if course in ['ST', 'STP']: return [(15, 100)]
        if course in ['230L', '230LP']: return [(15, 100)]
    return []

def filter_by_evaluation_range(df, course, scenario):
    """ë°ì´í„°ë¥¼ í”„ë¡¬í”„íŠ¸ì— ëª…ì‹œëœ í‰ê°€ êµ¬ê°„ì— ë”°ë¼ í•„í„°ë§í•©ë‹ˆë‹¤."""
    if df is None or df.empty:
        return None
    
    eval_ranges = get_evaluation_ranges(course, scenario)
    if not eval_ranges:
        st.warning(f"{course} - {scenario}ì— ëŒ€í•œ í‰ê°€ êµ¬ê°„ì´ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return df

    combined_mask = pd.Series(False, index=df.index)
    for start, end in eval_ranges:
        mask = df['Distance'].between(start, end, inclusive='both')
        combined_mask |= mask
    
    df_filtered = df[combined_mask].copy() 
    
    st.info(f"í‰ê°€ êµ¬ê°„ {eval_ranges} ë‚´ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤. (ì´ {len(df_filtered)}ê°œ ìƒ˜í”Œ)")
    
    if df_filtered.empty:
        st.error("ì§€ì •ëœ í‰ê°€ êµ¬ê°„ ë‚´ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì£¼í–‰ ê²½ë¡œ ë˜ëŠ” ì½”ìŠ¤ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
        return None
        
    return df_filtered

def get_baseline(course, scenario, distance_axis):
    """'ADB ê²°ê³¼ ë¶„ì„ í”„ë¡¬í”„íŠ¸.md' ê¸°ë°˜ìœ¼ë¡œ ì½”ìŠ¤ë³„ ê¸°ì¤€ì„ (Upper Limit) ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    limit = pd.Series(np.nan, index=distance_axis.index)
    
    course_eval_ranges = get_evaluation_ranges(course, scenario)
    if not course_eval_ranges:
        return limit

    if scenario == 'Oncoming':
        criteria = {
            (15, 29.9): 3.1,
            (30, 59.9): 1.8,
            (60, 119.9): 0.6,
            (120, 220): 0.3
        }
    elif scenario == 'Preceding':
        criteria = {
            (30, 59.9): 18.9,
            (60, 119.9): 4.0
        }
    else:
        criteria = {}

    for course_start, course_end in course_eval_ranges:
        for (crit_start, crit_end), lx_val in criteria.items():
            apply_start = max(course_start, crit_start)
            apply_end = min(course_end, crit_end)
            
            if apply_start <= apply_end:
                limit[distance_axis.between(apply_start, apply_end, inclusive='both')] = lx_val
                
    return limit

def generate_report(df, course, scenario, fig_path=None, fig_curve=None, fig_pitch=None, csv_filename=None, tdms_filename=None, df_pitch_exceeded=None, pitch_stats=None):
    """ë¶„ì„ ê²°ê³¼ë¥¼ 'report test.xlsx' ì–‘ì‹ì— ë§ê²Œ Excel ë³´ê³ ì„œë¡œ ìƒì„±í•˜ê³ , NG í–‰ì„ ê°•ì¡° í‘œì‹œí•©ë‹ˆë‹¤."""
    if df is None or df.empty:
        st.warning("ë³´ê³ ì„œë¥¼ ìƒì„±í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None, None

    # --- 1. Data Processing for Report Table ---
    df_sorted = df.sort_values('Distance').reset_index(drop=True)
    if df_sorted['Distance'].duplicated().any():
        df_sorted = df_sorted.groupby('Distance').mean(numeric_only=True).reset_index()

    min_dist, max_dist = df_sorted['Distance'].min(), df_sorted['Distance'].max()
    if pd.isna(min_dist) or pd.isna(max_dist):
        return None, None

    distance_grid = np.arange(np.floor(min_dist), np.ceil(max_dist), DISTANCE_GRID_INTERVAL)
    df_indexed = df_sorted.set_index('Distance')
    new_index = pd.Index(distance_grid, name='Distance')
    
    df_numeric_indexed = df_indexed.select_dtypes(include=np.number)
    df_gridded = df_numeric_indexed.reindex(df_numeric_indexed.index.union(new_index)) \
                                   .interpolate(method='index') \
                                   .reindex(new_index)
    df_gridded.reset_index(inplace=True)

    if 'UpperLimit' not in df_gridded.columns:
        df_gridded['UpperLimit'] = get_baseline(course, scenario, df_gridded['Distance'])

    report_data = []
    if scenario == 'Oncoming':
        mid_gain_cols = [f'Middle Gain {i}' for i in [4, 5]]
    elif scenario == 'Preceding':
        mid_gain_cols = [f'Middle Gain {i}' for i in range(6, 11)]
    else:
        mid_gain_cols = []

    # --- NG Row Identification for LAW DATA highlighting ---
    ng_row_mask = pd.Series(False, index=df_gridded.index)
    for col in mid_gain_cols:
        if col in df_gridded.columns:
            exceeds = (df_gridded[col] > df_gridded['UpperLimit']) & df_gridded['UpperLimit'].notna()
            ng_row_mask |= exceeds

    for col in mid_gain_cols:
        if col in df_gridded.columns and not df_gridded[col].isnull().all():
            measurement = df_gridded[col]
            limit = df_gridded['UpperLimit']
            
            eval_mask = limit.notna()
            eval_ranges = get_evaluation_ranges(course, scenario)
            course_total_eval_dist = sum(end - start for start, end in eval_ranges)

            exceeds = (measurement > limit) & eval_mask
            ng_points = exceeds.sum()
            ng_dist = ng_points * DISTANCE_GRID_INTERVAL

            # Calculate Exceed Rate
            exceed_rate = (ng_dist / course_total_eval_dist) * 100 if course_total_eval_dist > 0 else 0
            
            overall = "OK" if ng_points == 0 else "NG"
            
            worst_exceed, worst_dist, rate_at_worst_point = 0, np.nan, 0
            if overall == "NG":
                diff = measurement - limit
                worst_exceed_series = diff[exceeds]
                if not worst_exceed_series.empty:
                    worst_idx = worst_exceed_series.idxmax()
                    worst_exceed = worst_exceed_series.max()
                    worst_dist = df_gridded.loc[worst_idx, 'Distance']
                    
                    limit_at_worst_point = limit.loc[worst_idx]
                    if limit_at_worst_point > 0:
                        rate_at_worst_point = (worst_exceed / limit_at_worst_point) * 100
                    else:
                        rate_at_worst_point = np.inf # Handle case where limit is 0 to avoid division error

            report_data.append({
                "Channel": col, 
                "Overall (OK/NG)": overall, 
                "ì´ˆê³¼ìœ¨ (%)": f"{exceed_rate:.2f}",
                "ëˆ„ì  NG ê±°ë¦¬ (m)": f"{ng_dist:.2f}",
                "WorstExceed (lx)": f"{worst_exceed:.2f}", 
                "WorstDist (m)": f"{worst_dist:.2f}",
                "ìµœëŒ€ ì´ˆê³¼ì§€ì  ì´ˆê³¼ìœ¨ (%)": f"{rate_at_worst_point:.2f}"
            })
    df_report = pd.DataFrame(report_data)

    # --- 2. Create Excel Report using xlsxwriter ---
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        workbook = writer.book
        report_sheet = workbook.add_worksheet('Report')

        # --- Define Formats ---
        title_format = workbook.add_format({'bold': True, 'font_size': 20, 'align': 'center'})
        subtitle_format = workbook.add_format({'bold': True, 'font_size': 12})
        header_format = workbook.add_format({'bold': True, 'font_size': 11, 'align': 'center', 'valign': 'vcenter', 'bg_color': '#DDEBF7', 'border': 1})
        ok_format = workbook.add_format({'bold': True, 'font_size': 11, 'align': 'center', 'bg_color': '#C6EFCE', 'font_color': '#006100'})
        ng_format = workbook.add_format({'bold': True, 'font_size': 11, 'align': 'center', 'bg_color': '#FFC7CE', 'font_color': '#9C0006'})
        cell_format = workbook.add_format({'align': 'center'})
        info_header_format = workbook.add_format({'bold': True, 'align': 'right'})
        yellow_format = workbook.add_format({'bg_color': '#FFFF00'}) # Yellow for NG rows

        # --- Sheet 1: Formatted Report ---
        report_sheet.set_column('A:A', 25) # Adjusted for new explanation header
        report_sheet.set_column('B:G', 20)
        report_sheet.merge_range('A1:G1', 'ADB Test Report', title_format)

        # --- Test Information ---
        report_sheet.write('A3', 'Test Information', subtitle_format)
        report_sheet.write('A4', 'CSV File:', info_header_format)
        report_sheet.write('B4', csv_filename)
        report_sheet.write('A5', 'TDMS File:', info_header_format)
        report_sheet.write('B5', tdms_filename)
        report_sheet.write('A6', 'Course:', info_header_format)
        report_sheet.write('B6', course)
        report_sheet.write('A7', 'Scenario:', info_header_format)
        report_sheet.write('B7', scenario)

        # --- Add user-editable fields ---
        report_sheet.write('D4', 'ì‹œí—˜ ì¼ì:', info_header_format)
        report_sheet.write('D5', 'í‰ê°€ì:', info_header_format)
        report_sheet.write('D6', 'ì°¨ì¢…ëª…:', info_header_format)
        editable_format = workbook.add_format({'bg_color': '#F2F2F2', 'border': 1})
        report_sheet.write('E4', '', editable_format)
        report_sheet.write('E5', '', editable_format)
        report_sheet.write('E6', '', editable_format)

        # --- Pitch Statistics ---
        if pitch_stats:
            pitch_info_start_row = report_sheet.dim_row + 2
            report_sheet.write(f'A{pitch_info_start_row}', 'Pitch Statistics', subtitle_format)
            report_sheet.write(f'A{pitch_info_start_row + 1}', 'Avg Pitch (deg):', info_header_format)
            report_sheet.write(f'B{pitch_info_start_row + 1}', f"{pitch_stats['avg']:.3f}")
            report_sheet.write(f'A{pitch_info_start_row + 2}', 'Max Pitch (deg):', info_header_format)
            report_sheet.write(f'B{pitch_info_start_row + 2}', f"{pitch_stats['max']:.3f}")
            report_sheet.write(f'A{pitch_info_start_row + 3}', 'Min Pitch (deg):', info_header_format)
            report_sheet.write(f'B{pitch_info_start_row + 3}', f"{pitch_stats['min']:.3f}")

        # --- Overall Result Summary ---
        summary_start_row = 9
        final_judgment = "OK" if "NG" not in df_report['Overall (OK/NG)'].unique() else "NG"
        report_sheet.write(f'A{summary_start_row}', 'Final Result:', subtitle_format)
        report_sheet.merge_range(f'B{summary_start_row}:C{summary_start_row}', final_judgment, ok_format if final_judgment == "OK" else ng_format)
        
        # --- Detailed Result Table ---
        table_start_row = summary_start_row + 2
        headers = ["Channel", "Overall (OK/NG)", "ì´ˆê³¼ìœ¨ (%)", "ëˆ„ì  NG ê±°ë¦¬ (m)", "WorstExceed (lx)", "WorstDist (m)", "ìµœëŒ€ ì´ˆê³¼ì§€ì  ì´ˆê³¼ìœ¨ (%)"]
        for col_num, header in enumerate(headers):
            report_sheet.write(table_start_row, col_num, header, header_format)
        
        if not df_report.empty:
            for row_num, row_data in df_report.iterrows():
                current_row = table_start_row + row_num + 1
                report_sheet.write(current_row, 0, row_data['Channel'], cell_format)
                report_sheet.write(current_row, 1, row_data['Overall (OK/NG)'], ok_format if row_data['Overall (OK/NG)'] == "OK" else ng_format)
                
                exceed_rate = float(row_data['ì´ˆê³¼ìœ¨ (%)'])
                ng_dist = float(row_data['ëˆ„ì  NG ê±°ë¦¬ (m)'])
                worst_exceed = float(row_data['WorstExceed (lx)'])
                worst_dist = float(row_data['WorstDist (m)'])
                rate_at_worst = float(row_data['ìµœëŒ€ ì´ˆê³¼ì§€ì  ì´ˆê³¼ìœ¨ (%)'])

                report_sheet.write_number(current_row, 2, exceed_rate, cell_format)
                report_sheet.write_number(current_row, 3, ng_dist, cell_format)

                if np.isnan(worst_exceed) or worst_exceed == 0:
                    report_sheet.write(current_row, 4, "-", cell_format)
                else:
                    report_sheet.write_number(current_row, 4, worst_exceed, cell_format)

                if np.isnan(worst_dist):
                    report_sheet.write(current_row, 5, "-", cell_format)
                else:
                    report_sheet.write_number(current_row, 5, worst_dist, cell_format)

                if np.isnan(rate_at_worst) or rate_at_worst == 0:
                    report_sheet.write(current_row, 6, "-", cell_format)
                else:
                    report_sheet.write_number(current_row, 6, rate_at_worst, cell_format)

        # --- Add Explanations ---
        explanation_start_row = table_start_row + len(df_report) + 2
        report_sheet.merge_range(f'A{explanation_start_row}:G{explanation_start_row}', 'ìš©ì–´ ì„¤ëª…', subtitle_format)
        
        explanation_format = workbook.add_format({'font_size': 10, 'valign': 'top', 'text_wrap': True, 'border': 1})
        explanation_header_format = workbook.add_format({'bold': True, 'font_size': 10, 'align': 'center', 'valign': 'vcenter', 'bg_color': '#F2F2F2', 'border': 1})

        report_sheet.write(f'A{explanation_start_row + 1}', 'ì´ˆê³¼ìœ¨ (%)', explanation_header_format)
        report_sheet.merge_range(f'B{explanation_start_row + 1}:G{explanation_start_row + 1}', 'ì „ì²´ í‰ê°€ êµ¬ê°„ ê±°ë¦¬ ì¤‘, ë²•ê·œ ì¡°ë„ ê¸°ì¤€ì„ ì´ˆê³¼í•œ ëˆ„ì  ê±°ë¦¬ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤.', explanation_format)
        
        report_sheet.write(f'A{explanation_start_row + 2}', 'ìµœëŒ€ ì´ˆê³¼ì§€ì  ì´ˆê³¼ìœ¨ (%)', explanation_header_format)
        report_sheet.merge_range(f'B{explanation_start_row + 2}:G{explanation_start_row + 2}', 'ë²•ê·œ ê¸°ì¤€ì„ ê°€ì¥ í¬ê²Œ ìœ„ë°˜í•œ ì§€ì ì—ì„œ, ê¸°ì¤€ê°’ ëŒ€ë¹„ ì´ˆê³¼ëœ ì¡°ë„ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤. ((ì¸¡ì •ê°’ - ê¸°ì¤€ê°’) / ê¸°ì¤€ê°’ * 100)', explanation_format)

        # --- Place Graphs ---
        graph_start_row = explanation_start_row + 4 # Adjust graph start row
        if fig_path:
            report_sheet.write(f'A{graph_start_row}', 'Graph 1: Driving Path', subtitle_format)
            img_path_data = io.BytesIO()
            fig_path.write_image(img_path_data, format='png', width=720, height=405)
            report_sheet.insert_image(f'B{graph_start_row + 1}', 'path_graph.png', {'image_data': img_path_data})

        if fig_curve:
            curve_graph_start_row = graph_start_row + 23
            report_sheet.write(f'A{curve_graph_start_row}', 'Graph 2: Illuminance vs. Distance', subtitle_format)
            img_curve_data = io.BytesIO()
            fig_curve.write_image(img_curve_data, format='png', width=720, height=405)
            report_sheet.insert_image(f'B{curve_graph_start_row + 1}', 'curve_graph.png', {'image_data': img_curve_data})
        
        if fig_pitch:
            pitch_graph_start_row = graph_start_row + 46
            report_sheet.write(f'A{pitch_graph_start_row}', 'Graph 3: Pitch vs. Distance', subtitle_format)
            img_pitch_data = io.BytesIO()
            fig_pitch.write_image(img_pitch_data, format='png', width=720, height=405)
            report_sheet.insert_image(f'B{pitch_graph_start_row + 1}', 'pitch_graph.png', {'image_data': img_pitch_data})

        # --- Sheet 2: Raw Data with Highlighting ---
        raw_data_sheet = workbook.add_worksheet('LAW DATA')
        raw_data_sheet.set_column('A:Z', 15)

        # Write header
        for col_num, value in enumerate(df_gridded.columns.values):
            raw_data_sheet.write(0, col_num, value, header_format)

        # Write data with conditional formatting
        for row_num, row_data in df_gridded.iterrows():
            row_format = yellow_format if ng_row_mask[row_num] else None
            for col_num, cell_value in enumerate(row_data):
                if pd.isna(cell_value):
                    raw_data_sheet.write_blank(row_num + 1, col_num, None, row_format)
                elif isinstance(cell_value, (int, float)):
                    raw_data_sheet.write_number(row_num + 1, col_num, cell_value, row_format)
                else:
                    raw_data_sheet.write(row_num + 1, col_num, cell_value, row_format)

        # --- Sheet 3: Pitch Exceedance Data ---
        if df_pitch_exceeded is not None and not df_pitch_exceeded.empty:
            pitch_sheet = workbook.add_worksheet('Pitch ì´ˆê³¼ ì§€ì ')
            pitch_sheet.set_column('A:Z', 18)

            pitch_sheet.merge_range('A1:D1', 'Pitch ê¸°ì¤€ì„  (Avg Â± 0.3 deg) ì´ˆê³¼ ì§€ì ', title_format)

            if scenario == 'Oncoming':
                relevant_channels = [f'Middle Gain {i}' for i in [4, 5] if f'Middle Gain {i}' in df_pitch_exceeded.columns]
            else: # Preceding
                relevant_channels = [f'Middle Gain {i}' for i in range(6, 11) if f'Middle Gain {i}' in df_pitch_exceeded.columns]
            
            display_cols = ['Distance', 'Pitch'] + relevant_channels
            
            for col_num, value in enumerate(display_cols):
                pitch_sheet.write(2, col_num, value, header_format)

            for row_num, row_data in df_pitch_exceeded[display_cols].reset_index(drop=True).iterrows():
                for col_num, cell_value in enumerate(row_data):
                    if pd.isna(cell_value):
                        pitch_sheet.write_blank(row_num + 3, col_num, None, cell_format)
                    elif isinstance(cell_value, (int, float)):
                        pitch_sheet.write_number(row_num + 3, col_num, cell_value, cell_format)
                    else:
                        pitch_sheet.write(row_num + 3, col_num, cell_value, cell_format)

    return output.getvalue(), df_report

def plot_distance_illuminance_curve(df, course, scenario, file_name, fig=None, xaxis_max=None, color=None):
    """ê±°ë¦¬-ì¡°ë„ ê³¡ì„  ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ê¸°ì¡´ ê·¸ë˜í”„ì— ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
    if fig is None:
        fig = go.Figure()
        # Add layout elements only for the first plot
        fig.update_layout(title=f"ê±°ë¦¬-ì¡°ë„ ê³¡ì„  ({scenario} - {course})", xaxis_title="ê±°ë¦¬ (m)", yaxis_title="ì¡°ë„ (lx)", legend_title="íŒŒì¼ - ì±„ë„", font_family="Noto Sans CJK KR")
        fig.update_xaxes(range=[0, xaxis_max])
        fig.update_yaxes(type="linear", autorange=True)

        # Add vrects and Upper Limit only once
        eval_ranges = get_evaluation_ranges(course, scenario)
        vrect_colors = px.colors.qualitative.Pastel
        for i, (start, end) in enumerate(eval_ranges):
            fig.add_vrect(x0=start, x1=end, fillcolor=vrect_colors[i % len(vrect_colors)], opacity=0.15, layer="below", line_width=0, annotation_text=f"í‰ê°€êµ¬ê°„ {i+1}", annotation_position="top left")
        
        temp_df_for_limit = df.copy()
        if 'UpperLimit' not in temp_df_for_limit.columns:
             temp_df_for_limit['UpperLimit'] = get_baseline(course, scenario, temp_df_for_limit['Distance'])
        df_limit = temp_df_for_limit[['Distance', 'UpperLimit']].dropna().sort_values('Distance')
        fig.add_trace(go.Scatter(x=df_limit['Distance'], y=df_limit['UpperLimit'], mode='lines', name='Upper Limit', line=dict(color="black", dash="dot", width=2.5)))

    if 'UpperLimit' not in df.columns:
        df['UpperLimit'] = get_baseline(course, scenario, df['Distance'])

    if scenario == 'Oncoming': channels_to_plot = [4, 5]
    elif scenario == 'Preceding': channels_to_plot = [6, 7, 8, 9, 10]
    else: channels_to_plot = range(1, 11)

    for i in channels_to_plot:
        channel_name = f"Middle Gain {i}"
        if channel_name in df.columns and not df[channel_name].isnull().all():
            trace_name = f"{file_name} - {channel_name}"
            legend_group = f"{file_name}-{i}"
            fig.add_trace(go.Scatter(x=df['Distance'], y=df[channel_name], mode='lines', name=trace_name, legendgroup=legend_group, line=dict(color=color)))
            
            exceeded_line = df[channel_name].copy()
            exceeded_line[(df[channel_name] <= df['UpperLimit']) | (df['UpperLimit'].isnull())] = np.nan
            fig.add_trace(go.Scatter(x=df['Distance'], y=exceeded_line, mode='lines', name=f'{trace_name} ì´ˆê³¼', legendgroup=legend_group, showlegend=False, line=dict(color='red', width=2.5)))

    return fig

def plot_pitch_curve(df, file_name, course, scenario, fig=None, xaxis_max=None, color=None, pitch_stats=None):
    """ê±°ë¦¬-Pitch ê³¡ì„  ê·¸ë˜í”„ë¥¼ ìƒì„±í•˜ê±°ë‚˜ ê¸°ì¡´ ê·¸ë˜í”„ì— ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ê³ , í†µê³„ ì°¸ì¡°ì„ ì„ ê·¸ë¦½ë‹ˆë‹¤."""
    if 'Pitch' not in df.columns or df['Pitch'].isnull().all():
        st.warning(f"[{file_name}] Pitch ë°ì´í„°ê°€ ì—†ì–´ ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return fig # Return original figure if no data

    if fig is None:
        fig = go.Figure()
        fig.update_layout(
            font_family="Noto Sans CJK KR",
            title="ê±°ë¦¬-Pitch ë³€í™” (ìœ íš¨êµ¬ê°„ í†µê³„ ê¸°ë°˜ ì°¸ì¡°ì„ )",
            xaxis_title="ê±°ë¦¬ (m)",
            yaxis_title="Pitch (deg)",
            showlegend=True,
            legend_title="File"
        )
        fig.update_xaxes(range=[0, xaxis_max])

        # Add vrects for evaluation ranges
        eval_ranges = get_evaluation_ranges(course, scenario)
        vrect_colors = px.colors.qualitative.Pastel
        for i, (start, end) in enumerate(eval_ranges):
            fig.add_vrect(x0=start, x1=end, fillcolor=vrect_colors[i % len(vrect_colors)], opacity=0.15, layer="below", line_width=0, annotation_text=f"í‰ê°€êµ¬ê°„ {i+1}", annotation_position="top left")

    # Add main pitch trace
    fig.add_trace(go.Scatter(
        x=df['Distance'],
        y=df['Pitch'],
        mode='lines',
        name=f'{file_name}',
        line=dict(color=color),
        legendgroup=file_name
    ))

    # Add statistics lines if available
    if pitch_stats:
        avg_pitch = pitch_stats['avg']
        
        # Average line
        fig.add_trace(go.Scatter(
            x=df['Distance'], y=[avg_pitch] * len(df),
            mode='lines',
            name=f'Avg Pitch ({avg_pitch:.3f})',
            line=dict(color=color, dash='dot'),
            legendgroup=file_name,
            showlegend=True
        ))
        # +/- 0.3 lines
        fig.add_trace(go.Scatter(
            x=df['Distance'], y=[avg_pitch + 0.3] * len(df),
            mode='lines',
            name='+0.3',
            line=dict(color=color, dash='dash', width=1),
            legendgroup=file_name,
            showlegend=False # Hide from legend to reduce clutter
        ))
        fig.add_trace(go.Scatter(
            x=df['Distance'], y=[avg_pitch - 0.3] * len(df),
            mode='lines',
            name='-0.3',
            line=dict(color=color, dash='dash', width=1),
            legendgroup=file_name,
            showlegend=False # Hide from legend to reduce clutter
        ))

    return fig

# --- 3. Streamlit UI ---

def main():
    st.set_page_config(layout="wide")
    st.title("ë¶ë¯¸ ADB ì‹¤ì°¨í‰ê°€ ë°ì´í„° ë¶„ì„ ë„êµ¬ (v9 - ìë™ ì‹œê°„ ë™ê¸°í™”)")

    # --- Session State Initialization ---
    if 'analysis_done' not in st.session_state:
        st.session_state.analysis_done = False
        st.session_state.fig_path = None
        st.session_state.fig_curve = None
        st.session_state.fig_pitch = None
        st.session_state.all_summaries = []
        st.session_state.report_downloads = []

    # --- Sidebar Setup ---
    st.sidebar.header("ì…ë ¥ ì„¤ì •")
    vehicle_csv_files = st.sidebar.file_uploader("1. ì£¼í–‰ ë°ì´í„° ì—…ë¡œë“œ (CSV)", type="csv", accept_multiple_files=True)
    illuminance_tdms_files = st.sidebar.file_uploader("2. ì¡°ë„ ë°ì´í„° ì—…ë¡œë“œ (TDMS)", type="tdms", accept_multiple_files=True)

    st.sidebar.header("ë¶„ì„ ì„¤ì •")
    
    scenario = st.sidebar.selectbox(
        "ì‹œë‚˜ë¦¬ì˜¤ ì„ íƒ",
        options=["Oncoming", "Preceding"],
        index=0,
        key='scenario'
    )

    oncoming_courses = ['ST', '100L', '230L', '230R', '370L', '370R']
    preceding_display_map = { 'ST': 'STP', '230L': '230LP' }

    if scenario == 'Oncoming':
        course_options = [c for c in oncoming_courses if c in ZERO_MAP]
        course = st.sidebar.selectbox("ì½”ìŠ¤ ì„ íƒ", options=course_options, index=0, key='course_oncoming')
    else: # Preceding
        course_display_options = list(preceding_display_map.keys())
        course_selected_display = st.sidebar.selectbox("ì½”ìŠ¤ ì„ íƒ", options=course_display_options, index=0, key='course_preceding')
        course = preceding_display_map[course_selected_display]



    manual_offset = st.sidebar.number_input("ìˆ˜ë™ ì¡°ë„ ì˜¤í”„ì…‹ ì¶”ê°€ (lx)", value=0.0, step=0.001, format="%.3f", key='manual_illuminance_offset')
    
    st.sidebar.header("ì¶œë ¥ ì„¤ì •")
    save_path_input = st.sidebar.text_input("ê²°ê³¼ ì €ì¥ í´ë” (ì„ íƒ ì‚¬í•­)", placeholder="ì˜ˆ: C:\\Users\\Desktop")

    if st.sidebar.button("ë¶„ì„ ì‹œì‘", type="primary"):
        st.session_state.analysis_done = False # Reset on new analysis
        if not vehicle_csv_files or not illuminance_tdms_files:
            st.sidebar.warning("ì£¼í–‰ ë° ì¡°ë„ íŒŒì¼ì„ ëª¨ë‘ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        elif len(vehicle_csv_files) != len(illuminance_tdms_files):
            st.sidebar.error("ì£¼í–‰(CSV)ê³¼ ì¡°ë„(TDMS) íŒŒì¼ì˜ ê°œìˆ˜ê°€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.")
        else:
            with st.spinner("ì „ì²´ ë¶„ì„ ì§„í–‰ ì¤‘..."):
                # --- Setup for multiple file processing ---
                fig_path = go.Figure()
                fig_curve = None
                fig_pitch = None
                all_summaries = []
                report_downloads = []
                plot_colors = px.colors.qualitative.Plotly

                # --- Main processing loop ---
                for i, (vehicle_csv_file, illuminance_tdms_file) in enumerate(zip(vehicle_csv_files, illuminance_tdms_files)):
                    csv_filename = vehicle_csv_file.name
                    tdms_filename = illuminance_tdms_file.name
                    file_identifier = csv_filename.replace('.csv', '')
                    color = plot_colors[i % len(plot_colors)]

                    st.markdown(f"--- \n ## ë¶„ì„ ({i+1}/{len(vehicle_csv_files)}): {file_identifier}")
                    
                    df_vehicle = load_vehicle_csv(vehicle_csv_file)
                    df_illuminance = load_illuminance_tdms(illuminance_tdms_file)

                    if df_vehicle is None or df_illuminance is None:
                        st.error(f"'{file_identifier}' íŒŒì¼ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ íŒŒì¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                        continue

                    time_offset = TIME_OFFSET_SECONDS
                    df_synced = synchronize_data(df_vehicle, df_illuminance, time_offset)

                    if df_synced is None:
                        st.error(f"'{file_identifier}' ë°ì´í„° ë™ê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ íŒŒì¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                        continue
                    
                    # --- ìë™ ë° ìˆ˜ë™ ì¡°ë„ ì˜¤í”„ì…‹ ê³„ì‚° ë° ì ìš© ---
                    zero_point = ZERO_MAP.get(course)
                    # 1. ì˜¤í”„ì…‹ ê³„ì‚°ì„ ìœ„í•´ ì„ì‹œë¡œ ê±°ë¦¬ ê³„ì‚°
                    temp_df_with_dist = calculate_distance(df_synced.copy(), zero_point)

                    auto_offset = 0.0
                    if temp_df_with_dist is not None:
                        min_dist_idx = temp_df_with_dist['Distance'].idxmin()
                        zero_point_data = temp_df_with_dist.loc[min_dist_idx]
                        
                        if scenario == 'Oncoming':
                            channels_for_ambient = [f'Middle Gain {i}' for i in [4, 5]]
                        else: # Preceding
                            channels_for_ambient = [f'Middle Gain {i}' for i in range(6, 11)]
                        
                        ambient_values = [zero_point_data.get(ch) for ch in channels_for_ambient if pd.notna(zero_point_data.get(ch))]

                        if ambient_values:
                            ambient_baseline = min(ambient_values)
                            auto_offset = -ambient_baseline
                        else:
                            st.warning("0m ì§€ì  ì¡°ë„ê°’ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ìë™ ì˜¤í”„ì…‹ì„ ì ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        st.error(f"'{file_identifier}' ê±°ë¦¬ ê³„ì‚° ì‹¤íŒ¨ë¡œ ì˜¤í”„ì…‹ì„ ì ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                    # 2. ìµœì¢… ì˜¤í”„ì…‹ ê³„ì‚° ë° ì •ë³´ í‘œì‹œ
                    total_offset = auto_offset + manual_offset
                    
                    st.write("##### ì¡°ë„ ì˜¤í”„ì…‹ ì •ë³´")
                    c1, c2, c3 = st.columns(3)
                    c1.metric(label="ê³„ì‚°ëœ ìë™ ì˜¤í”„ì…‹", value=f"{auto_offset:.3f} lx")
                    c2.metric(label="ìˆ˜ë™ ì¶”ê°€ ì˜¤í”„ì…‹", value=f"{manual_offset:.3f} lx")
                    c3.metric(label="ìµœì¢… ì ìš© ì˜¤í”„ì…‹", value=f"{total_offset:.3f} lx", delta=f"{manual_offset:.3f} lx")


                    # 3. ì›ë³¸ ë°ì´í„°ì— ìµœì¢… ì˜¤í”„ì…‹ ì ìš©
                    if total_offset != 0.0:
                        mid_gain_cols = [f'Middle Gain {i}' for i in range(1, 11)]
                        for col in mid_gain_cols:
                            if col in df_synced.columns:
                                df_synced[col] = pd.to_numeric(df_synced[col], errors='coerce') + total_offset
                        st.info(f"ì¡°ë„ ê°’ì— ìµœì¢… ì˜¤í”„ì…‹ {total_offset:.3f} lxë¥¼ ì ìš©í–ˆìŠµë‹ˆë‹¤.")
                    
                    # 4. ì˜¤í”„ì…‹ì´ ì ìš©ëœ ë°ì´í„°ë¡œ ìµœì¢… ê±°ë¦¬ ê³„ì‚°
                    df_with_dist = calculate_distance(df_synced, zero_point)
                    
                    if df_with_dist is None or df_with_dist.empty:
                        st.warning(f"'{file_identifier}' ê±°ë¦¬ ê³„ì‚°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        continue

                    min_dist_idx = df_with_dist['Distance'].idxmin()
                    if isinstance(min_dist_idx, pd.Series): min_dist_idx = min_dist_idx.iloc[0]
                    df_approaching = df_with_dist.loc[:min_dist_idx].copy()
                    st.info(f"ì°¨ëŸ‰ ì ‘ê·¼ êµ¬ê°„ ë°ì´í„° í•„í„°ë§ ì™„ë£Œ. (ì´ {len(df_approaching)}ê°œ ìƒ˜í”Œ)")

                    df_final_for_report = filter_by_evaluation_range(df_approaching, course, scenario)

                    # --- Pitch í†µê³„ ê³„ì‚° (ìœ íš¨ êµ¬ê°„ ê¸°ì¤€) ---
                    pitch_stats = None
                    if df_final_for_report is not None and not df_final_for_report.empty and 'Pitch' in df_final_for_report.columns and not df_final_for_report['Pitch'].isnull().all():
                        pitch_series = df_final_for_report['Pitch']
                        pitch_stats = {
                            'avg': pitch_series.mean(),
                            'max': pitch_series.max(),
                            'min': pitch_series.min()
                        }
                        st.markdown(f"##### [{file_identifier}] ìœ íš¨êµ¬ê°„ Pitch í†µê³„")
                        col1, col2, col3 = st.columns(3)
                        col1.metric(label="í‰ê·  Pitch (deg)", value=f"{pitch_stats['avg']:.3f}")
                        col2.metric(label="ìµœëŒ€ Pitch (deg)", value=f"{pitch_stats['max']:.3f}")
                        col3.metric(label="ìµœì†Œ Pitch (deg)", value=f"{pitch_stats['min']:.3f}")
                    else:
                        st.info(f"[{file_identifier}] ìœ íš¨êµ¬ê°„ì— Pitch ë°ì´í„°ê°€ ì—†ì–´ í†µê³„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                    # --- Pitch ê¸°ì¤€ì„  ì´ˆê³¼ ì§€ì  ë¶„ì„ ---
                    df_pitch_exceeded = None # Initialize
                    if pitch_stats and df_final_for_report is not None:
                        avg_pitch = pitch_stats['avg']
                        upper_bound = avg_pitch + 0.3
                        lower_bound = avg_pitch - 0.3

                        exceed_mask = (df_final_for_report['Pitch'] > upper_bound) | (df_final_for_report['Pitch'] < lower_bound)
                        df_pitch_exceeded = df_final_for_report[exceed_mask]

                        st.markdown(f"##### [{file_identifier}] Pitch ê¸°ì¤€ì„  (Avg Â± 0.3 deg) ì´ˆê³¼ ì§€ì ")
                        if not df_pitch_exceeded.empty:
                            if scenario == 'Oncoming':
                                relevant_channels = [f'Middle Gain {i}' for i in [4, 5] if f'Middle Gain {i}' in df_pitch_exceeded.columns]
                            else: # Preceding
                                relevant_channels = [f'Middle Gain {i}' for i in range(6, 11) if f'Middle Gain {i}' in df_pitch_exceeded.columns]
                            
                            display_cols = ['Distance', 'Pitch'] + relevant_channels
                            
                            st.warning(f"{len(df_pitch_exceeded)}ê°œì˜ ì§€ì ì—ì„œ Pitch ê¸°ì¤€ì„ ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
                            st.dataframe(df_pitch_exceeded[display_cols].style.format({
                                'Distance': '{:.2f}m',
                                'Pitch': '{:.3f}Â°',
                                **{ch: '{:.3f} lx' for ch in relevant_channels}
                            }))
                        else:
                            st.success("ëª¨ë“  ìœ íš¨êµ¬ê°„ì—ì„œ Pitch ê¸°ì¤€ì„ ì„ ë§Œì¡±í•©ë‹ˆë‹¤.")


                    fig_path_single = px.line(df_vehicle, x='PosLocalX', y='PosLocalY', title=f'ì£¼í–‰ ê²½ë¡œ ({file_identifier})')
                    fig_path_single.update_traces(line=dict(color='lightgrey'), name='ì „ì²´ ê²½ë¡œ', showlegend=True)
                    if df_final_for_report is not None and not df_final_for_report.empty:
                        fig_path_single.add_trace(go.Scatter(x=df_final_for_report['PosLocalX'], y=df_final_for_report['PosLocalY'], mode='markers', name='ë¶„ì„ êµ¬ê°„', marker=dict(color='red', size=3)))
                    fig_path_single.update_layout(xaxis_title="PosLocalX (m)", yaxis_title="PosLocalY (m)", font_family="Noto Sans CJK KR")
                    fig_path_single.update_yaxes(scaleanchor="x", scaleratio=1)

                    eval_ranges = get_evaluation_ranges(course, scenario)
                    xaxis_max = max(end for start, end in eval_ranges) * 1.1 if eval_ranges else None
                    
                    fig_curve_single = None
                    fig_pitch_single = None
                    if not df_approaching.empty:
                        fig_curve_single = plot_distance_illuminance_curve(df_approaching, course, scenario, file_identifier, fig=None, xaxis_max=xaxis_max, color=color)
                        fig_pitch_single = plot_pitch_curve(df_approaching.copy(), file_identifier, course, scenario, fig=None, xaxis_max=xaxis_max, color=color, pitch_stats=pitch_stats)

                    fig_path.add_trace(go.Scatter(x=df_vehicle['PosLocalX'], y=df_vehicle['PosLocalY'], mode='lines', name=f'{file_identifier} (ì „ì²´)', legendgroup=f'group{i}', line=dict(color=color, dash='dot')))
                    if df_final_for_report is not None and not df_final_for_report.empty:
                        fig_path.add_trace(go.Scatter(x=df_final_for_report['PosLocalX'], y=df_final_for_report['PosLocalY'], mode='lines', name=f'{file_identifier} (ë¶„ì„)', legendgroup=f'group{i}', line=dict(color=color, width=3)))

                    if not df_approaching.empty:
                        fig_curve = plot_distance_illuminance_curve(df_approaching, course, scenario, file_identifier, fig=fig_curve, xaxis_max=xaxis_max, color=color)
                        fig_pitch = plot_pitch_curve(df_approaching, file_identifier, course, scenario, fig=fig_pitch, xaxis_max=xaxis_max, color=color, pitch_stats=pitch_stats)

                    if df_final_for_report is not None and not df_final_for_report.empty:
                        report_bytes, df_summary = generate_report(df_final_for_report, course, scenario, fig_path=fig_path_single, fig_curve=fig_curve_single, fig_pitch=fig_pitch_single, csv_filename=csv_filename, tdms_filename=tdms_filename, df_pitch_exceeded=df_pitch_exceeded, pitch_stats=pitch_stats)
                        if df_summary is not None and not df_summary.empty: all_summaries.append((file_identifier, df_summary))
                        if report_bytes: report_downloads.append((file_identifier, course, scenario, report_bytes))
                    else:
                        st.warning(f"[{file_identifier}] ìœ íš¨ í‰ê°€ êµ¬ê°„ ë‚´ì— ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

                # --- Store results in session state ---
                st.session_state.fig_path = fig_path
                st.session_state.fig_curve = fig_curve
                st.session_state.fig_pitch = fig_pitch
                st.session_state.all_summaries = all_summaries
                st.session_state.report_downloads = report_downloads
                st.session_state.analysis_done = True
                st.session_state.save_path = save_path_input # Store save path

    # --- Display results from session state ---
    if st.session_state.analysis_done:
        st.markdown("--- \n # ì¢…í•© ê²°ê³¼")

        st.subheader("ì£¼í–‰ ê²½ë¡œ")
        st.plotly_chart(st.session_state.fig_path, use_container_width=True)

        st.subheader("ê±°ë¦¬-ì¡°ë„ ê³¡ì„ ")
        if st.session_state.fig_curve:
            st.plotly_chart(st.session_state.fig_curve, use_container_width=True)
        
        st.subheader("ê±°ë¦¬-Pitch ë³€í™”")
        if st.session_state.fig_pitch:
            st.plotly_chart(st.session_state.fig_pitch, use_container_width=True)

        st.subheader("ìµœì¢… íŒì • ìš”ì•½")
        with st.expander("ìš©ì–´ ì„¤ëª… ë³´ê¸°"):
            st.markdown("""
            - **ì´ˆê³¼ìœ¨ (%)**: ì „ì²´ í‰ê°€ êµ¬ê°„ ê±°ë¦¬ ì¤‘, ë²•ê·œ ì¡°ë„ ê¸°ì¤€ì„ ì´ˆê³¼í•œ ëˆ„ì  ê±°ë¦¬ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤.
            - **ìµœëŒ€ì§€ì  ì´ˆê³¼ìœ¨ (%)**: ë²•ê·œ ê¸°ì¤€ì„ ê°€ì¥ í¬ê²Œ ìœ„ë°˜í•œ ì§€ì ì—ì„œ, ê¸°ì¤€ê°’ ëŒ€ë¹„ ì´ˆê³¼ëœ ì¡°ë„ì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤. (`(ì¸¡ì •ê°’ - ê¸°ì¤€ê°’) / ê¸°ì¤€ê°’ * 100`)
            """
            )

        for file_identifier, df_summary in st.session_state.all_summaries:
            st.markdown(f"#### {file_identifier}")
            df_summary.rename(columns={'Overall (OK/NG)': 'íŒì •', 'WorstExceed (lx)': 'ìµœëŒ€ ì´ˆê³¼ëŸ‰ (lx)', 'WorstDist (m)': 'ìµœëŒ€ ì´ˆê³¼ ì§€ì  (m)', 'ìµœëŒ€ ì´ˆê³¼ì§€ì  ì´ˆê³¼ìœ¨ (%)': 'ìµœëŒ€ì§€ì  ì´ˆê³¼ìœ¨ (%)'}, inplace=True)
            if 'ì´ˆê³¼ìœ¨ (%)' not in df_summary.columns: df_summary['ì´ˆê³¼ìœ¨ (%)'] = '0.0'
            if 'ëˆ„ì  NG ê±°ë¦¬ (m)' not in df_summary.columns: df_summary['ëˆ„ì  NG ê±°ë¦¬ (m)'] = '0.0'
            for col in ['ì´ˆê³¼ìœ¨ (%)', 'ëˆ„ì  NG ê±°ë¦¬ (m)', 'ìµœëŒ€ ì´ˆê³¼ëŸ‰ (lx)', 'ìµœëŒ€ ì´ˆê³¼ ì§€ì  (m)', 'ìµœëŒ€ì§€ì  ì´ˆê³¼ìœ¨ (%)']:
                df_summary[col] = pd.to_numeric(df_summary[col], errors='coerce')
            final_judgment = "OK" if "NG" not in df_summary['íŒì •'].unique() else "NG"
            if final_judgment == "OK": st.success(f"**{file_identifier} ìµœì¢… íŒì •: OK**")
            else: st.error(f"**{file_identifier} ìµœì¢… íŒì •: NG**")
            display_cols = ['Channel', 'íŒì •', 'ì´ˆê³¼ìœ¨ (%)', 'ëˆ„ì  NG ê±°ë¦¬ (m)', 'ìµœëŒ€ ì´ˆê³¼ëŸ‰ (lx)', 'ìµœëŒ€ ì´ˆê³¼ ì§€ì  (m)', 'ìµœëŒ€ì§€ì  ì´ˆê³¼ìœ¨ (%)']
            st.dataframe(df_summary[display_cols].style.format({'ì´ˆê³¼ìœ¨ (%)': '{:.2f}%', 'ëˆ„ì  NG ê±°ë¦¬ (m)': '{:.2f}', 'ìµœëŒ€ ì´ˆê³¼ëŸ‰ (lx)': '{:.2f}', 'ìµœëŒ€ ì´ˆê³¼ ì§€ì  (m)': '{:.2f}', 'ìµœëŒ€ì§€ì  ì´ˆê³¼ìœ¨ (%)': '{:.2f}%'}))

        if st.session_state.report_downloads:
            st.subheader("ë³´ê³ ì„œ ì €ì¥ ë° ë‹¤ìš´ë¡œë“œ")
            save_path = st.session_state.get('save_path', '')

            if st.button("ì„ íƒí•œ í´ë”ì— ëª¨ë“  ë¦¬í¬íŠ¸ ì €ì¥"):
                if save_path and os.path.isdir(save_path):
                    num_saved = 0
                    for file_identifier, course, scenario, report_bytes in st.session_state.report_downloads:
                        file_name = f"ADB_Report_{file_identifier}_{course}_{scenario}_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
                        try:
                            full_path = os.path.join(save_path, file_name)
                            with open(full_path, "wb") as f:
                                f.write(report_bytes)
                            st.success(f"ë³´ê³ ì„œ ì €ì¥ ì™„ë£Œ: `{full_path}`")
                            num_saved += 1
                        except Exception as e:
                            st.error(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
                    if num_saved > 0:
                        st.balloons()
                elif not save_path:
                    st.warning("ì €ì¥í•  í´ë” ê²½ë¡œë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
                else:
                    st.error(f"ì…ë ¥í•œ ê²½ë¡œ '{save_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ í´ë”ê°€ ì•„ë‹™ë‹ˆë‹¤.")

            st.markdown("--- ")
            st.write("ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ:")
            for file_identifier, course, scenario, report_bytes in st.session_state.report_downloads:
                file_name = f"ADB_Report_{file_identifier}_{course}_{scenario}_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
                st.download_button(
                    label=f"ğŸ“¥ {file_identifier} ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ",
                    data=report_bytes,
                    file_name=file_name,
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    key=f"download_{file_identifier}"
                )
    else:
        st.info("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ ì„¤ì •ì„ ë§ˆì¹œ í›„ 'ë¶„ì„ ì‹œì‘'ì„ í´ë¦­í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()